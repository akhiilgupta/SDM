package mypackage

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import scala.io.Source
import org.apache.spark.rdd.RDD


object Percentile {
  def main(args: Array[String]): Unit = {
    
    def findPercentile(data: RDD[Int], percentile: Double, count: Int): Int = {
      if(data.count() < 1){
        return -1;
      }
      else{
        val pivot = data.takeSample(false, 1).apply(0);
      
        val filteredData = data.filter(x => x <= pivot);
        if(filteredData.count() + count == percentile){
          return pivot;
        }
        else if(filteredData.count() + count < percentile){
          return findPercentile(data.filter(x => x > pivot), percentile, filteredData.count().toInt);
        }
        else{
          return findPercentile(filteredData, percentile, count);
        }
      }
    }
    
    val spConfig = (new SparkConf).setAppName("Spark Matrix Multiplication").setMaster("local[*]");
    val sc = new SparkContext(spConfig);
    val listOfLines = Source.fromFile("input_list.txt").getLines.toArray;
    val ls = listOfLines.map(_.toInt);
    
    val list = sc.parallelize(ls, 3);//.zipWithIndex().map(_.swap).collect().foreach(print);
    val perc25 = findPercentile(list ,.25*list.count().toInt,0);
    
    println(perc25);
    sc.stop();
  }
}