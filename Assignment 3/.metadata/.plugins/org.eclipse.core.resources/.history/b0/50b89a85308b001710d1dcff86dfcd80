package my_package

import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.SparkConf

class BookCluster(data: Seq[Seq[Double]], n_clusters: Int = 5, n_max_iteration: Int = 100){
  def kmeansFunction(sc:SparkContext):Unit = {
    
    val rdd = sc.parallelize(data);
    
    val vec = rdd.map(row => Vectors.dense(row.toArray));
    val x_clusters = KMeans.train(vec, n_clusters, n_max_iteration);
    val n_wsse = x_clusters.computeCost(vec);
    
    //println("Within Sum of Squared Errors = " + n_wsse);
    //println("Cluster centers = ");
    //x_clusters.clusterCenters.foreach(println);
    
    val l_cluster_assignment = x_clusters.predict(vec);
    val book1 = l_cluster_assignment.zipWithIndex().filter(elem => elem._1 == 0).map(x => x._2);
    val book2 = l_cluster_assignment.zipWithIndex().filter(elem => elem._1 == 1).map(x => x._2);
    
    val string1 = book1.collect.mkString(",").concat("\\n");
    val string2 = book2.collect.mkString(",");
    val str = string1.concat(string2);
    println(str);
    
    //sc.parallelize(str, 1).saveAsTextFile("output")
    
  }
}
