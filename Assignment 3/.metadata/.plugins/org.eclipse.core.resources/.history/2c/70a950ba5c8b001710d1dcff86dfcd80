package my_package

import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.Vectors

class BookCluster(data: Seq[Seq[Double]], num_clusters: Int, num_iteration: Int){
  def kmeansFunction(sc:SparkContext):Unit = {    
    
    val rdd = sc.parallelize(data);
    val vec = rdd.map(row => Vectors.dense(row.toArray));
    val x_clusters = KMeans.train(vec, 2, 5);
    val n_wsse = x_clusters.computeCost(vec);
    
    //x_clusters.clusterCenters.foreach(println);
    
    val l_cluster_assignment = x_clusters.predict(vec);
    
    val book1 = l_cluster_assignment.zipWithIndex().filter(elem => elem._1 == 0).map(x => x._2);
    val book2 = l_cluster_assignment.zipWithIndex().filter(elem => elem._1 == 1).map(x => x._2);
    sc.parallelize(l_cluster_assignment.collect).saveAsTextFile("test");
    sc.stop();
  }
}