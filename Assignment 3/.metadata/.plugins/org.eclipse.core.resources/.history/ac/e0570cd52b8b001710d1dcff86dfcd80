package my_package

import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.SparkConf

class BookCluster(data: Seq[Seq[Double]], n_clusters: Int = 5, n_max_iteration: Int = 20){
  def kmeansFunction(sc:SparkContext):Unit = {
    val n_clusters = 2
    val n_max_iteration = 20
    
    
    //val rdd = sc.parallelize(Seq(Seq(1, 2, 3), Seq(4, 5, 6), Seq(7, 8, 9)))
    //val transposed = sc.parallelize(rdd.collect.toSeq.transpose)
    
    //val a = data.mapValues(_.toSeq).toSeq.map(_._2);
    val rdd = sc.parallelize(data);
    
    val vec = rdd.map(row => Vectors.dense(row.toArray));
    val x_clusters = KMeans.train(vec, n_clusters, n_max_iteration);
    val n_wsse = x_clusters.computeCost(vec);
    
    println("Within Sum of Squared Errors = " + n_wsse);
    println("Cluster centers = ");
    x_clusters.clusterCenters.foreach(println);
    
    val l_cluster_assignment = x_clusters.predict(vec);
    val book1 = l_cluster_assignment.zipWithIndex().filter(elem => elem._2 == 0).map(x => x._2);
    val book2 = l_cluster_assignment.zipWithIndex().filter(elem => elem._2 == 1).map(x => x._2);
    
    println("Book1:");
    book1.foreach{x => print(x); print(" ")}
  }
}
