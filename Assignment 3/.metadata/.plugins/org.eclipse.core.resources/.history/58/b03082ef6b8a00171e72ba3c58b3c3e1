package my_package

import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.Vectors

class KMeans{
  def kmeansFunction(sc:SparkContext):Unit = {
   
  val c_path_in = "D:/sample.csv"
  val c_path_out ="D:/temp/kmeansOutput/"
  val n_clusters = 2
  val n_max_iteration = 20
   
  val r_csv = sc.textFile(c_path_in);
   
  val c_header = r_csv.first;
   
  val r_data = r_csv.filter(_(0) != c_header(0));
  val r_parsedData = r_data.map(s => Vectors.dense(s.split(",").map(_.toDouble))).cache()
   
  val x_clusters = KMeans.train(r_parsedData, n_clusters, n_max_iteration)
  val n_wsse = x_clusters.computeCost(r_parsedData)
   
  println("Within Sum of Squared Errors = " + n_wsse)
  println("Cluster centers = ")
  x_clusters.clusterCenters.foreach(println)
   
  val l_cluster_assignment = x_clusters.predict(r_parsedData)
  l_cluster_assignment.saveAsTextFile(c_path_out)
  }
}
