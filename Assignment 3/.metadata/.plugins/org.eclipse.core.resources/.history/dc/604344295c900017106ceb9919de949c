package my_package

import org.apache.spark.mllib.clustering.PowerIterationClustering
import org.apache.spark.rdd.RDD

class BookCluster(data: RDD[(Long, Long, Double)], num_clusters: Int, n_iteration: Int){
  val model = new PowerIterationClustering()
  .setK(num_clusters).setMaxIterations(n_iteration).setInitializationMode("degree").run(data);

  val clusters = model.assignments.collect().groupBy(_.cluster).mapValues(_.map(_.id));
  val assignments = clusters.toList.sortBy { case (k, v) => v.length }//.map(ele => (ele._1+1, ele._2.map(e => e+1)));
  val assignmentsStr = assignments.map { case (k, v) => s"$k -> ${v.sorted.mkString("[", ",", "]")}"}.mkString(", ");
  val sizesStr = assignments.map {_._2.length}.sorted.mkString("(", ",", ")");
  println(s"Books: $assignmentsStr\ncluster sizes: $sizesStr");
}