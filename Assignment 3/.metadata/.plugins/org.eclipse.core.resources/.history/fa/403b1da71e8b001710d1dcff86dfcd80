package my_package

import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.SparkConf

class BookCluster(data: Map[Int, Array[Double]]){
  def kmeansFunction(sc:SparkContext):Unit = {
    val n_clusters = 2
    val n_max_iteration = 20
    
    val rdd_data = sc.parallelize(data.toSeq, 4);
    
    val vec = rdd_data.map(row => Vectors.dense(row._2));
    
    val x_clusters = KMeans.train(vec, n_clusters, n_max_iteration);
    val n_wsse = x_clusters.computeCost(vec);
    
    println("Within Sum of Squared Errors = " + n_wsse);
    println("Cluster centers = ");
    x_clusters.clusterCenters.foreach(println);
    
    val l_cluster_assignment = x_clusters.predict(vec);
    l_cluster_assignment.saveAsTextFile("test");
  }
}
