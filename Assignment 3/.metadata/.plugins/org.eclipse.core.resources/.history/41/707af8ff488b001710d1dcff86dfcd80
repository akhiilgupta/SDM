package my_package

import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.SparkConf

class BookCluster(data: Seq[Seq[Double]], n_clusters: Int, n_max_iteration: Int){
  def kmeansFunction(sc:SparkContext):Unit = {    
    
    val rdd = sc.parallelize(data);
    
    val vec = rdd.map(row => Vectors.dense(row.toArray));
    val x_clusters = KMeans.train(vec, n_clusters, n_max_iteration);
    val n_wsse = x_clusters.computeCost(vec);
    
    /*println("Within Sum of Squared Errors = " + n_wsse);
    println("Cluster centers = ");
    x_clusters.clusterCenters.foreach(println);*/
    
    val l_cluster_assignment = x_clusters.predict(vec);
    
    val book1 = l_cluster_assignment.zipWithIndex().filter(elem => elem._1 == 0).map(x => x._2);
    val book2 = l_cluster_assignment.zipWithIndex().filter(elem => elem._1 == 1).map(x => x._2);
    
    sc.parallelize(l_cluster_assignment.collect).saveAsTextFile("test");
  }
}
