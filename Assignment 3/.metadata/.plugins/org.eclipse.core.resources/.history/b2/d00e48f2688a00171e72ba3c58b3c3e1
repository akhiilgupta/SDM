package my_package

import org.apache.spark.SparkConf
import java.nio.charset.CodingErrorAction
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import scala.io.Source
import scala.io.Codec

object LSH {
  def main(args: Array[String]): Unit = {
    //def splitInShingles(text: String): Array[String]{
      //return 0;
    //}
    val conf = new SparkConf().setAppName("LSH").setMaster("local[*]");
    val sc = new SparkContext(conf);
    
    implicit val codec = Codec("UTF-8");
    codec.onMalformedInput(CodingErrorAction.REPLACE);
    codec.onUnmappableCharacter(CodingErrorAction.REPLACE);
    
    val file_contents = Source.fromFile("Data.txt").getLines().drop(1).mkString.trim;
    
    val lines = file_contents.split("(?=\\t\\d{1,3}\\t)");
    val lines_trimmed = lines.map(x => x.replaceAll("""(?m)\s+$""", "").trim);
    val lines_split = lines_trimmed.map(line => line.split("\\t")).filter(x => x.size == 2)
        .map(x => (x(0).toInt, x(1)));
    
    val lines_cleaned = lines_split.map(x  => x._2.replaceAll("[\",\\.\\']", "").toLowerCase())
    
    var corpus = collection.mutable.HashSet[String]();
    
    def makeMap(text: String, map: collection.mutable.HashSet[String]): Unit = {
      var shingles = text.sliding(5, 1);
      shingles.foreach(sh => map+=sh );
    }
    
    //lines_cleaned.foreach(line => makeMap(line, corpus));
    
    val doc = lines_cleaned.take(5).toIndexedSeq.zipWithIndex;
    
    val lsh = new LSH1(5, 1000, 50, doc, 0.3);
    
    val string = lines_cleaned(4).toString();
    
    lsh.createHash();
    
    val seq = lsh.findSimilar(string);
    
    seq.foreach(println);

    sc.stop();
  }
}